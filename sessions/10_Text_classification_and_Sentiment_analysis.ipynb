{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10_Text_classification_and_Sentiment_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_a2hajcA99-K",
        "colab_type": "text"
      },
      "source": [
        "![BTS](https://github.com/vfp1/bts-mbds-data-science-foundations-2019/blob/master/sessions/img/Logo-BTS.jpg?raw=1)\n",
        "\n",
        "# Session 10: Text classification and Sentiment analysis\n",
        "\n",
        "### Victor F. Pajuelo Madrigal <victor.pajuelo@bts.tech> - Data Science Foundations (2019-11-07)\n",
        "\n",
        "Open this notebook in Google Colaboratory: [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vfp1/bts-mbds-data-science-foundations-2019/blob/master/sessions/10_Text_classification_and_Sentiment_analysis.ipynb)\n",
        "\n",
        "**Resources:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTIcPknwQz5z",
        "colab_type": "text"
      },
      "source": [
        "# Spacy installation\n",
        "\n",
        "```\n",
        "$ conda activate bts36\n",
        "$ conda install -c conda-forge spacy\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QHBivImfOvp",
        "colab_type": "text"
      },
      "source": [
        "## Import language models \n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "$ python -m spacy download en_core_web_sm\n",
        "$ python -m spacy download en\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPv81PAORYB7",
        "colab_type": "code",
        "outputId": "e2789e3c-8183-4124-de72-e4c385637ced",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_lg==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz (826.9MB)\n",
            "\u001b[K     |████████████████████████████████| 826.9MB 74.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.1.0-cp36-none-any.whl size=828255076 sha256=1ae59ff5604a75e9e585f78f7ec993f0ed77a60dbb836c594157353447d83d25\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-tbnlm3bw/wheels/b4/d7/70/426d313a459f82ed5e06cc36a50e2bb2f0ec5cb31d8e0bdf09\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mQRg-8jfzDI",
        "colab_type": "code",
        "outputId": "77fb2e93-b889-4a62-8e3a-437e86ea212e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "!python -m spacy download en"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmwJHdwEU4uM",
        "colab_type": "text"
      },
      "source": [
        "Once the model is downloaded and installed, we can load it as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuuFgOdRRWif",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgVUba0w6x-c",
        "colab_type": "text"
      },
      "source": [
        "# Text classification (Sentiment analysis pipeline) \n",
        "\n",
        "Today we will be using for first time an `scikit learn` pipeline to prepare the data and to classify it. \n",
        "\n",
        "We will start with three different datasets where we have a collection of user reactions:\n",
        "\n",
        "*   IMBD - Review of movies\n",
        "*   Amazon - Technology products user review\n",
        "*   Yelp - Restaurant food reviews\n",
        "\n",
        "The dataset is coded with a `0` when the review is bad and with a `1` when the review is good. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPjDVEKswX2h",
        "colab_type": "text"
      },
      "source": [
        "## ETL: Extract Transform Load\n",
        "\n",
        "The first process in our pipeline is an ETL one, i.e. Extract, Transform and Load. We will prepare our dataset to be cleaned and to be passed to the processing pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjNbpaNnL66V",
        "colab_type": "text"
      },
      "source": [
        "### Load datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIo4cCiZzYyR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "d2564120-23ce-41d6-db62-ff72a79a9ecc"
      },
      "source": [
        "!wget \"https://github.com/vfp1/bts-mbds-data-science-foundations-2019/raw/master/sessions/data/amazon_cells_labelled.txt\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-07 05:04:13--  https://github.com/vfp1/bts-mbds-data-science-foundations-2019/raw/master/sessions/data/amazon_cells_labelled.txt\n",
            "Resolving github.com (github.com)... 192.30.253.112\n",
            "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/vfp1/bts-mbds-data-science-foundations-2019/master/sessions/data/amazon_cells_labelled.txt [following]\n",
            "--2019-11-07 05:04:13--  https://raw.githubusercontent.com/vfp1/bts-mbds-data-science-foundations-2019/master/sessions/data/amazon_cells_labelled.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 58226 (57K) [text/plain]\n",
            "Saving to: ‘amazon_cells_labelled.txt’\n",
            "\n",
            "\r          amazon_ce   0%[                    ]       0  --.-KB/s               \ramazon_cells_labell 100%[===================>]  56.86K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2019-11-07 05:04:13 (2.25 MB/s) - ‘amazon_cells_labelled.txt’ saved [58226/58226]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkwnIoX-zb09",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "9af8b2b4-6ec6-4e18-a7b0-2a3efc7111d7"
      },
      "source": [
        "!wget \"https://github.com/vfp1/bts-mbds-data-science-foundations-2019/raw/master/sessions/data/imdb_labelled.txt\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-07 05:04:14--  https://github.com/vfp1/bts-mbds-data-science-foundations-2019/raw/master/sessions/data/imdb_labelled.txt\n",
            "Resolving github.com (github.com)... 192.30.253.112\n",
            "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/vfp1/bts-mbds-data-science-foundations-2019/master/sessions/data/imdb_labelled.txt [following]\n",
            "--2019-11-07 05:04:14--  https://raw.githubusercontent.com/vfp1/bts-mbds-data-science-foundations-2019/master/sessions/data/imdb_labelled.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 85285 (83K) [text/plain]\n",
            "Saving to: ‘imdb_labelled.txt’\n",
            "\n",
            "\rimdb_labelled.txt     0%[                    ]       0  --.-KB/s               \rimdb_labelled.txt   100%[===================>]  83.29K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2019-11-07 05:04:14 (3.31 MB/s) - ‘imdb_labelled.txt’ saved [85285/85285]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4C1VBJQztQL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "47305867-cb72-44bc-e12e-224910a2089d"
      },
      "source": [
        "!wget \"https://github.com/vfp1/bts-mbds-data-science-foundations-2019/raw/master/sessions/data/yelp_labelled.txt\""
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-07 05:05:21--  https://github.com/vfp1/bts-mbds-data-science-foundations-2019/raw/master/sessions/data/yelp_labelled.txt\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/vfp1/bts-mbds-data-science-foundations-2019/master/sessions/data/yelp_labelled.txt [following]\n",
            "--2019-11-07 05:05:21--  https://raw.githubusercontent.com/vfp1/bts-mbds-data-science-foundations-2019/master/sessions/data/yelp_labelled.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 61320 (60K) [text/plain]\n",
            "Saving to: ‘yelp_labelled.txt’\n",
            "\n",
            "\ryelp_labelled.txt     0%[                    ]       0  --.-KB/s               \ryelp_labelled.txt   100%[===================>]  59.88K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2019-11-07 05:05:21 (2.42 MB/s) - ‘yelp_labelled.txt’ saved [61320/61320]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWoLiqmlMYu3",
        "colab_type": "text"
      },
      "source": [
        "Once we have ready data that is labelled (please take a minute to look at the source data), we can read it with `Pandas`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrNMgzyqwV0k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load our dataset\n",
        "df_yelp = pd.read_table('yelp_labelled.txt')\n",
        "df_imdb = pd.read_table('imdb_labelled.txt')\n",
        "df_amz = pd.read_table('amazon_cells_labelled.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEFur13CMqqT",
        "colab_type": "text"
      },
      "source": [
        "Then we can concatenate all the datasets, so we create a single dataset that contains movie, technology and food reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-N4hnN10ABD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "2732e753-381d-40d8-88a8-aa216ae0334c"
      },
      "source": [
        "# Concatenate our Datasets\n",
        "frames = [df_yelp,df_imdb,df_amz]\n",
        "frames"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[                              Wow... Loved this place.  1\n",
              " 0                                   Crust is not good.  0\n",
              " 1            Not tasty and the texture was just nasty.  0\n",
              " 2    Stopped by during the late May bank holiday of...  1\n",
              " 3    The selection on the menu was great and so wer...  1\n",
              " 4       Now I am getting angry and I want my damn pho.  0\n",
              " ..                                                 ... ..\n",
              " 994  I think food should have flavor and texture an...  0\n",
              " 995                           Appetite instantly gone.  0\n",
              " 996  Overall I was not impressed and would not go b...  0\n",
              " 997  The whole experience was underwhelming, and I ...  0\n",
              " 998  Then, as if I hadn't wasted enough of my life ...  0\n",
              " \n",
              " [999 rows x 2 columns],\n",
              "     A very, very, very slow-moving, aimless movie about a distressed, drifting young man.    0\n",
              " 0    Not sure who was more lost - the flat characte...                                       0\n",
              " 1    Attempting artiness with black & white and cle...                                       0\n",
              " 2         Very little music or anything to speak of.                                         0\n",
              " 3    The best scene in the movie was when Gerardo i...                                       1\n",
              " 4    The rest of the movie lacks art, charm, meanin...                                       0\n",
              " ..                                                 ...                                      ..\n",
              " 742  I just got bored watching Jessice Lange take h...                                       0\n",
              " 743  Unfortunately, any virtue in this film's produ...                                       0\n",
              " 744                   In a word, it is embarrassing.                                         0\n",
              " 745                               Exceptionally bad!                                         0\n",
              " 746  All in all its an insult to one's intelligence...                                       0\n",
              " \n",
              " [747 rows x 2 columns],\n",
              "     So there is no way for me to plug it in here in the US unless I go by a converter.  0\n",
              " 0                          Good case, Excellent value.                                  1\n",
              " 1                               Great for the jawbone.                                  1\n",
              " 2    Tied to charger for conversations lasting more...                                  0\n",
              " 3                                    The mic is great.                                  1\n",
              " 4    I have to jiggle the plug to get it to line up...                                  0\n",
              " ..                                                 ...                                 ..\n",
              " 994  The screen does get smudged easily because it ...                                  0\n",
              " 995  What a piece of junk.. I lose more calls on th...                                  0\n",
              " 996                       Item Does Not Match Picture.                                  0\n",
              " 997  The only thing that disappoint me is the infra...                                  0\n",
              " 998  You can not answer calls with the unit, never ...                                  0\n",
              " \n",
              " [999 rows x 2 columns]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vySlA41a0DFV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "f9f3c993-27dd-4b54-be8a-983f874fb45d"
      },
      "source": [
        "# Renaming Column Headers\n",
        "for colname in frames:\n",
        "    colname.columns = [\"Message\",\"Target\"]\n",
        "frames"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[                                               Message  Target\n",
              " 0                                   Crust is not good.       0\n",
              " 1            Not tasty and the texture was just nasty.       0\n",
              " 2    Stopped by during the late May bank holiday of...       1\n",
              " 3    The selection on the menu was great and so wer...       1\n",
              " 4       Now I am getting angry and I want my damn pho.       0\n",
              " ..                                                 ...     ...\n",
              " 994  I think food should have flavor and texture an...       0\n",
              " 995                           Appetite instantly gone.       0\n",
              " 996  Overall I was not impressed and would not go b...       0\n",
              " 997  The whole experience was underwhelming, and I ...       0\n",
              " 998  Then, as if I hadn't wasted enough of my life ...       0\n",
              " \n",
              " [999 rows x 2 columns],\n",
              "                                                Message  Target\n",
              " 0    Not sure who was more lost - the flat characte...       0\n",
              " 1    Attempting artiness with black & white and cle...       0\n",
              " 2         Very little music or anything to speak of.         0\n",
              " 3    The best scene in the movie was when Gerardo i...       1\n",
              " 4    The rest of the movie lacks art, charm, meanin...       0\n",
              " ..                                                 ...     ...\n",
              " 742  I just got bored watching Jessice Lange take h...       0\n",
              " 743  Unfortunately, any virtue in this film's produ...       0\n",
              " 744                   In a word, it is embarrassing.         0\n",
              " 745                               Exceptionally bad!         0\n",
              " 746  All in all its an insult to one's intelligence...       0\n",
              " \n",
              " [747 rows x 2 columns],\n",
              "                                                Message  Target\n",
              " 0                          Good case, Excellent value.       1\n",
              " 1                               Great for the jawbone.       1\n",
              " 2    Tied to charger for conversations lasting more...       0\n",
              " 3                                    The mic is great.       1\n",
              " 4    I have to jiggle the plug to get it to line up...       0\n",
              " ..                                                 ...     ...\n",
              " 994  The screen does get smudged easily because it ...       0\n",
              " 995  What a piece of junk.. I lose more calls on th...       0\n",
              " 996                       Item Does Not Match Picture.       0\n",
              " 997  The only thing that disappoint me is the infra...       0\n",
              " 998  You can not answer calls with the unit, never ...       0\n",
              " \n",
              " [999 rows x 2 columns]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Xfe8Q6P0MmU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Assign a Key to Make it Easier\n",
        "keys = ['Yelp','IMDB','Amazon']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2sPI4PJ0rTM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "fdff82b1-0926-4955-cf8d-394c055ef289"
      },
      "source": [
        "# Merge or Concat our Datasets\n",
        "df = pd.concat(frames,keys=keys)\n",
        "df"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Message</th>\n",
              "      <th>Target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">Yelp</th>\n",
              "      <th>0</th>\n",
              "      <td>Crust is not good.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Not tasty and the texture was just nasty.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Stopped by during the late May bank holiday of...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The selection on the menu was great and so wer...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Now I am getting angry and I want my damn pho.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">Amazon</th>\n",
              "      <th>994</th>\n",
              "      <td>The screen does get smudged easily because it ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>What a piece of junk.. I lose more calls on th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>Item Does Not Match Picture.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>The only thing that disappoint me is the infra...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>You can not answer calls with the unit, never ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2745 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                      Message  Target\n",
              "Yelp   0                                   Crust is not good.       0\n",
              "       1            Not tasty and the texture was just nasty.       0\n",
              "       2    Stopped by during the late May bank holiday of...       1\n",
              "       3    The selection on the menu was great and so wer...       1\n",
              "       4       Now I am getting angry and I want my damn pho.       0\n",
              "...                                                       ...     ...\n",
              "Amazon 994  The screen does get smudged easily because it ...       0\n",
              "       995  What a piece of junk.. I lose more calls on th...       0\n",
              "       996                       Item Does Not Match Picture.       0\n",
              "       997  The only thing that disappoint me is the infra...       0\n",
              "       998  You can not answer calls with the unit, never ...       0\n",
              "\n",
              "[2745 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4GCpOIr0tTL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "82112afb-64ed-422a-a919-008edaab9ebd"
      },
      "source": [
        "# Length and Shape \n",
        "df.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2745, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nqa_6_Qg1Q84",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "80c4e474-08bc-4bef-c1ed-9a3fa301a85a"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Message</th>\n",
              "      <th>Target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">Yelp</th>\n",
              "      <th>0</th>\n",
              "      <td>Crust is not good.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Not tasty and the texture was just nasty.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Stopped by during the late May bank holiday of...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The selection on the menu was great and so wer...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Now I am getting angry and I want my damn pho.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  Message  Target\n",
              "Yelp 0                                 Crust is not good.       0\n",
              "     1          Not tasty and the texture was just nasty.       0\n",
              "     2  Stopped by during the late May bank holiday of...       1\n",
              "     3  The selection on the menu was great and so wer...       1\n",
              "     4     Now I am getting angry and I want my damn pho.       0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8VAxzOOOOHg",
        "colab_type": "text"
      },
      "source": [
        "Last, we can save our raw dataset as a CSV file in our system."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5r1hTfmu1S-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv(\"sentimentdataset.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjB-NyQY2PV8",
        "colab_type": "text"
      },
      "source": [
        "### Cleaning dataset with spaCy\n",
        "\n",
        "* Removing Stopwords\n",
        "* Removing punctuation\n",
        "* Lemmatizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q203n2nd1XHr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "959ae63e-dd7e-467f-eadb-f53f526682fa"
      },
      "source": [
        "# Data Cleaning\n",
        "df.columns"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Message', 'Target'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1Z96tgM2RZf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5021b40e-b8e7-4cba-9dd6-e12065c32209"
      },
      "source": [
        "# Checking for Missing Values\n",
        "df.isnull().sum()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Message    0\n",
              "Target     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk3r4SFFPXin",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "0b0e8427-55db-4fb5-c27a-ec1b43186849"
      },
      "source": [
        "# Checking for the balance of our dataset\n",
        "df.info()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "MultiIndex: 2745 entries, (Yelp, 0) to (Amazon, 998)\n",
            "Data columns (total 2 columns):\n",
            "Message    2745 non-null object\n",
            "Target     2745 non-null int64\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 58.8+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLoinBPEQWXd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "866522e4-bd7b-4222-c335-2b9824d0b389"
      },
      "source": [
        "# Checking for the balance of our dataset\n",
        "df.Target.value_counts()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    1385\n",
              "0    1360\n",
              "Name: Target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZU4cfxLMQ19g",
        "colab_type": "text"
      },
      "source": [
        "#### Tokenizing our dataset with spaCy\n",
        "\n",
        "We will clean data that we do not need, like stopwords, punctuation and such from our dataset.\n",
        "\n",
        "This time we will also import the `string` dataset which has a good list of punctuation symbols.\n",
        "\n",
        "The function we will create will input a sentence, and processing into tokens, doing lemmatization, lowercasing, removing stopwords and avoiding punctuation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRnfSE837BEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.lang.en import English\n",
        "\n",
        "# Create our list of punctuation marks\n",
        "punctuations = string.punctuation\n",
        "\n",
        "# Create our list of stopwords\n",
        "nlp = spacy.load('en')\n",
        "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "parser = English()\n",
        "\n",
        "# Creating our tokenizer function\n",
        "def spacy_tokenizer(sentence):\n",
        "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
        "    mytokens = parser(sentence)\n",
        "\n",
        "    # Lemmatizing each token and converting each token into lowercase\n",
        "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
        "\n",
        "    # Removing stop words\n",
        "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
        "\n",
        "    # return preprocessed list of tokens\n",
        "    return mytokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKvqrz_MRrPX",
        "colab_type": "text"
      },
      "source": [
        "#### Defining a Transformer\n",
        "\n",
        "Transformers are trained products from networks which are heavily used in language processing (currently with BERT in Google translate). They are extremely good because they apply attention based models to look at sequences such as text.\n",
        "\n",
        "We will be using the class `TransformerMixin` from `scikit learn` to create our own class transformer.\n",
        "\n",
        "Our class will override the `transform`, `fit` and `get_params` from the main function and greate our own. We will also pass a function that remove the spaces and converts the text into lowercase for an easier analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qdNzR2tTTLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.base import TransformerMixin \n",
        "\n",
        "# This function will clean the text\n",
        "def clean_text(text):     \n",
        "    return text.strip().lower()\n",
        "    \n",
        "#Custom transformer using spaCy \n",
        "class predictors(TransformerMixin):\n",
        "\n",
        "    def transform(self, X, **transform_params):\n",
        "        return [clean_text(text) for text in X]\n",
        "\n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        return self\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I5UuJdRTqaU",
        "colab_type": "text"
      },
      "source": [
        "## Feature engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAWVXDtJT2LO",
        "colab_type": "text"
      },
      "source": [
        "### Vectorization with Bag of Words and TF-IDF\n",
        "\n",
        "When we classify text, we end up with text snippets matched with their respective labels. However, we need to represent our text in something that can be represented numerically. Classifying text in positive and negative labels is called **sentiment analysis**.\n",
        "\n",
        "There are different tools for that, i.e. **Bag of Words** and **TF-IDF**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qLwtSW1VGe_",
        "colab_type": "text"
      },
      "source": [
        "#### Bag of Words\n",
        "\n",
        "The first one converts text into the matrix of occurrence of words within a given document. It focuses on whether given words occurred or not in the document, and it generates a matrix that we might see referred to as a BoW matrix or a document term matrix.\n",
        "\n",
        "We can generate a BoW matrix for our text data by using scikit-learn‘s CountVectorizer. In the code below, we’re telling CountVectorizer to use the custom spacy_tokenizer function we built as its tokenizer, and defining the ngram range we want.\n",
        "\n",
        "N-grams are combinations of adjacent words in a given text, where n is the number of words that incuded in the tokens. for example, in the sentence “Who will win the football world cup in 2022?” unigrams would be a sequence of single words such as “who”, “will”, “win” and so on. Bigrams would be a sequence of 2 contiguous words such as “who will”, “will win”, and so on. So the ngram_range parameter we’ll use in the code below sets the lower and upper bounds of the our ngrams (we’ll be using unigrams). Then we’ll assign the ngrams to bow_vector.\n",
        "\n",
        "*Source: Dataquest*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyuB0CqyVL9j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# We create our bag of words (bow) using our tokenizer and defining an ngram range\n",
        "bow = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9k6HXrVVdjI",
        "colab_type": "text"
      },
      "source": [
        "#### TF-IDF\n",
        "\n",
        "In short, TF-IDF is a way of normalizing the BOW by looking at each word's frequency in comparisson to the document frequency.\n",
        "\n",
        "We will skip the sweat from the pass class and use `scikit learn` TF-IDF functionality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROQ2nJ24WE4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Using Tfidf\n",
        "tfvectorizer = TfidfVectorizer(tokenizer = spacy_tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKrHywI07Oru",
        "colab_type": "text"
      },
      "source": [
        "## Train-Test split\n",
        "\n",
        "In machine learning, we always need to split our datasets into train and test. We will use one for training the model and another one to check how the model performs. Luckily, `sklearn` comes with an in-built function for this. \n",
        "\n",
        "The split is done randomly, but we can attribute a seed value to make it stable for developing purposes. The usually split is 20% test and 80% train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vj25sHdU7lUp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splitting Data Set\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpS5Ny4N7owV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Features and Labels\n",
        "X = df['Message']\n",
        "ylabels = df['Target']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEEkk1Dy7sXF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.2, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3UeT9tMW54Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a99a3560-1cac-483f-9145-f4deead6c24e"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2196,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsLxBYBeXHJd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "96b07985-411d-4abf-c460-63cd1af63334"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2196,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oN6OHHGXDME",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1ca2509b-26ae-4609-eb85-d411f342d59d"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(549,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnK7DmoLXKIz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2207c35e-1ad2-4b63-9ed2-4a7a532de1d6"
      },
      "source": [
        "y_test.shape"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(549,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLmInSq7Xdo_",
        "colab_type": "text"
      },
      "source": [
        "## The classifier\n",
        "\n",
        "With choosing a classifier, we are choosing the strategy for our model to learn. Since we are trying to do a classification (good and bad) we will need to choose algorithms that are classifiers. \n",
        "\n",
        "We can play with the [classifiers from sklearn](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uKRaZK07YQ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SVC classifier\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "classifier_SVC = LinearSVC(verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4hbTHjVY6l3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Logistic Regression Classifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "classifier_LG = LogisticRegression(verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79WVa9K8Y_n_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Multi layer perceptron\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "classifier_MLP =  MLPClassifier(verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEKrBsPcZUeW",
        "colab_type": "text"
      },
      "source": [
        "## The pipeline\n",
        "\n",
        "We are going to create an `sklearn` pipeline that:\n",
        "\n",
        "1. Clean and preprocess the text using our predictors class from above\n",
        "2. Vectorize the words with either BOW or TF-IDF to create word matrixes from our text.\n",
        "3. Load the classifier which performs the algorithm we have chosen to classify the sentiments.\n",
        "\n",
        "![alt text](https://www.dataquest.io/wp-content/uploads/2019/04/text-classification-python-spacy.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TYK5QHP7C6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efV8aitr7uZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the  pipeline to clean, tokenize, vectorize, and classify \n",
        "pipe = Pipeline([(\"cleaner\", predictors()),\n",
        "                 ('vectorizer', bow),\n",
        "                 ('classifier', classifier_SVC)], verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbwcL03c7zUg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "b70983a9-d5c6-40e3-d8db-4d38de9a149a"
      },
      "source": [
        "# Fit our data\n",
        "pipe.fit(X_train,y_train)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........... (step 1 of 3) Processing cleaner, total=   0.0s\n",
            "[Pipeline] ........ (step 2 of 3) Processing vectorizer, total=   0.8s\n",
            "[LibLinear][Pipeline] ........ (step 3 of 3) Processing classifier, total=   0.0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('cleaner', <__main__.predictors object at 0x7f4a72854be0>),\n",
              "                ('vectorizer',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=<function spacy_tokenizer at 0x7f4a8703aa60>,\n",
              "                                 vocabulary=None)),\n",
              "                ('classifier',\n",
              "                 LinearSVC(C=1.0, class_weight=None, dual=True,\n",
              "                           fit_intercept=True, intercept_scaling=1,\n",
              "                           loss='squared_hinge', max_iter=1000,\n",
              "                           multi_class='ovr', penalty='l2', random_state=None,\n",
              "                           tol=0.0001, verbose=True))],\n",
              "         verbose=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDanHkxda2lo",
        "colab_type": "text"
      },
      "source": [
        "## Model evaluation\n",
        "\n",
        "Now that we have evaluated our model, let's look at how it performs! First of all we need to predict the results of the test using our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpoVMnGl70J-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predicting with a test dataset\n",
        "sample_prediction = pipe.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mESM0gyVbS49",
        "colab_type": "text"
      },
      "source": [
        "Let's check the results for each sample:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JAQsJga76Cl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prediction Results\n",
        "# 1 = Positive review\n",
        "# 0 = Negative review\n",
        "for (sample,pred) in zip(X_test,sample_prediction):\n",
        "    print(sample,\"Prediction=>\",pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbvCgAV5bZg5",
        "colab_type": "text"
      },
      "source": [
        "Now we can evaluate the model using different metrics, so that we can look at the three main performance metrics:\n",
        "\n",
        "* **Accuracy**: refers to the percentage of the total predictions our model makes that are completely correct.\n",
        "* **Precision**: describes the ratio of true positives to true positives plus false positives in our predictions.\n",
        "* **Recall**: describes the ratio of true positives to true positives plus false negatives in our predictions.\n",
        "\n",
        "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/800px-Precisionrecall.svg.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BPT_w8M780x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4c7f42ae-b213-4c36-e753-ec3379954631"
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "# Model Accuracy\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, sample_prediction))\n",
        "print(\"Precision:\",metrics.precision_score(y_test, sample_prediction))\n",
        "print(\"Recall:\",metrics.recall_score(y_test, sample_prediction))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7978142076502732\n",
            "Precision: 0.8215613382899628\n",
            "Recall: 0.778169014084507\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxVsPPracl4F",
        "colab_type": "text"
      },
      "source": [
        "## Let's use our model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPuRY1ZG9YRj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bf895cc2-4b3c-4f65-ba24-502328116e56"
      },
      "source": [
        "# Another random review\n",
        "pipe.predict([\"This was a great movie\"])"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqEiLfeO9q6e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9548ca20-5eb9-44b7-a6f4-c6060d4a7a90"
      },
      "source": [
        "example = [\"I do enjoy my job\",\n",
        " \"What a poor product!,I will have to get a new one\",\n",
        " \"I feel amazing!\",\n",
        " \"This class sucks\"]\n",
        "\n",
        "pipe.predict(example)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co9gND7o91Xo",
        "colab_type": "text"
      },
      "source": [
        "# Your turn\n",
        "\n",
        "Compare results with another approach:\n",
        "\n",
        "*   Try another vectorizer\n",
        "*   Try another train/test split\n",
        "*   Try another algorithm\n",
        "*   Try changing the parameters of the algorithm (more on that in class)\n",
        "*   If you feel hardcore: try [another dataset](https://lionbridge.ai/datasets/15-free-sentiment-analysis-datasets-for-machine-learning/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqhvJmYjdNL8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}